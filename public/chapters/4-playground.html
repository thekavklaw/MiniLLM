<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 4: The Playground ‚Äî MiniLLM</title>
  <link rel="stylesheet" href="/css/style.css">
</head>
<body data-chapter="4">
  <nav class="chapter-nav">
    <a href="/">‚Üê Home</a>
    <a href="/chapters/3-layers.html">‚Üê Prev</a>
    <span class="nav-title">Chapter 4: Playground</span>
    <a href="/chapters/5-words.html">Next ‚Üí</a>
    </nav>
  <div class="chapter-content">
    <div class="chapter-header fade-in-up">
      <div class="chapter-number">Chapter 4</div>
      <h1>The Playground</h1>
      <p>Build any network you want. Choose a dataset, tweak the architecture, and watch it learn in real time.</p>
    </div>

    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <!-- SECTION 1: Welcome to the Playground -->
    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <div class="section fade-in-up delay-1">
      <h2>üéÆ Welcome to Your Neural Network Sandbox</h2>

      <p>This is where everything from the previous chapters comes together. The playground below is a <strong>sandbox</strong> ‚Äî a place where you can experiment freely, break things, and learn by doing. There are no wrong answers here. Try wild combinations! Make the network fail spectacularly! That's how you build intuition.</p>

      <p>üéØ <strong>Your goal:</strong> By the end of this chapter, you should be able to pick a dataset, design a network architecture, choose sensible settings, and successfully train the network to classify the data. If you can do that for the Spiral dataset, you're officially doing machine learning!</p>

      <p>Real machine learning engineers spend a <em>huge</em> amount of time doing exactly this ‚Äî tweaking settings, running experiments, watching what happens, and building an instinct for what works. This playground gives you that same experience, compressed into a few minutes instead of a few years. Every great ML practitioner started by playing.</p>

      <p>Before you start clicking, let's take a tour of what you're looking at. The playground has <strong>three panels</strong>, each showing you something different about the network:</p>

      <h3>üìã Left Panel ‚Äî Configuration</h3>
      <p>This is your control center. You'll choose what kind of data the network should learn, how big the network should be, and how it should learn. Think of it as the "settings" for your experiment. Every option is explained in detail below ‚Äî don't worry if the names sound intimidating, they're simpler than they sound.</p>
      <p>The left panel has four cards: <strong>Dataset</strong> (pick your puzzle), <strong>Architecture</strong> (design the network shape), <strong>Settings</strong> (fine-tune learning behavior), and <strong>Controls</strong> (play/pause/reset). You'll also see live stats at the bottom showing the current epoch (training step), loss (how wrong the network is), and parameter count (total number of knobs the network is adjusting).</p>

      <h3>üé® Center Panel ‚Äî The Decision Boundary</h3>
      <p>This is the main event! The large colorful canvas shows you <strong>what the network "sees."</strong> Here's how to read it:</p>
      <ul>
        <li><strong>Purple/dark regions</strong> = the network predicts "class 1" here</li>
        <li><strong>Light/pale regions</strong> = the network predicts "class 0" here</li>
        <li><strong>Colored dots</strong> = the actual training data. Each dot belongs to one of two classes.</li>
        <li>A well-trained network will have purple regions surrounding class-1 dots and light regions surrounding class-0 dots.</li>
      </ul>
      <p>As the network trains, watch the colors shift and flow like liquid ‚Äî that's the network literally <em>changing its mind</em> about how to classify different regions of space. Early on, the boundary is usually a simple line or blob. As training progresses, it gets more detailed, curving around individual data points. This is the network learning!</p>
      <p>If you see the boundary getting <em>too</em> complex ‚Äî curving wildly between individual dots ‚Äî that's a sign of <strong>overfitting</strong> (the network memorizing instead of learning). We'll talk about how to fix that with regularization below.</p>

      <h3>üìä Right Panel ‚Äî Network Structure &amp; Loss</h3>
      <p>The top card shows a <strong>diagram of your network</strong> ‚Äî circles are neurons, lines are connections (weights). The thicker or more colorful a connection, the stronger that weight is. When you change the architecture (add layers, adjust neurons), this diagram updates instantly so you can see what you're building.</p>
      <p>Below the network diagram is the <strong>loss curve</strong> ‚Äî this is your progress tracker. The x-axis is time (training epochs) and the y-axis is how wrong the network is. You want this line to go <strong>down and stay down</strong>. When it flattens near zero, your network has learned the pattern. If it bounces around instead of going down, your learning rate is probably too high. If it barely moves, your learning rate is too low or your network is too small for the problem.</p>
      <p>Below both of those is a <strong>narration box</strong> that gives you a plain-English description of what's happening during training. It'll tell you when the network is just guessing, when it's making progress, and when it's nailed the dataset. Think of it as your personal ML coach, commenting on your network's performance in real time.</p>

      <p>Together, these three panels give you a complete picture: <strong>what you configured</strong> (left), <strong>what the network learned</strong> (center), and <strong>how it got there</strong> (right). This is essentially the same monitoring setup that real ML engineers use, just simplified and visual.</p>
    </div>

    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <!-- SECTION 2: Understanding Every Setting -->
    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <div class="section fade-in-up delay-1">
      <h2>‚öôÔ∏è Understanding Every Setting</h2>

      <p>Let's go through each option so you know exactly what you're tweaking. Understanding these settings is the difference between "randomly clicking buttons" and "actually doing machine learning." Each one corresponds to a real decision that ML engineers make every day.</p>

      <p>Don't try to memorize all of this before playing ‚Äî read through it once, then come back and re-read specific sections as you experiment. Learning by doing is the whole point of this chapter.</p>

      <h3>üìê Dataset</h3>
      <p>The <strong>dataset</strong> is the shape of data the network needs to learn to separate. Think of it as the "puzzle" you're giving the network. Each dataset places dots on the canvas in a different pattern ‚Äî some dots are one color (class 0) and others are a different color (class 1). The network's job is to figure out the rule that separates the two groups and draw a boundary between them.</p>
      <p>The datasets are ordered roughly by difficulty. Start with Circle to build confidence, then work your way up to the harder ones:</p>
      <ul>
        <li><strong>Circle</strong> ‚Äî One class forms a ring around the other. Like a bullseye target. A simple network can handle this.</li>
        <li><strong>XOR</strong> ‚Äî The diagonal pattern from Chapter 3. Dots in opposite corners belong to the same class. Needs at least one hidden layer.</li>
        <li><strong>Spiral</strong> ‚Äî Two classes wound around each other like a cinnamon roll. This is <em>hard</em> ‚Äî the boundary needs to twist and turn, requiring multiple layers.</li>
        <li><strong>Gaussian</strong> ‚Äî Two overlapping blobs of dots. Like two clouds that partially overlap. Some points near the boundary are genuinely ambiguous ‚Äî even the "perfect" classifier would get some wrong. This teaches an important lesson: sometimes there's no perfect answer.</li>
        <li><strong>Checkerboard</strong> ‚Äî A grid pattern like a chess board. Each square alternates between classes. Very hard ‚Äî the network needs to learn multiple separate regions simultaneously. This is a great stress test for your network architecture.</li>
      </ul>
      <p><strong>Difficulty ranking:</strong> Circle (‚≠ê) ‚Üí Gaussian (‚≠ê‚≠ê) ‚Üí XOR (‚≠ê‚≠ê‚≠ê) ‚Üí Spiral (‚≠ê‚≠ê‚≠ê‚≠ê) ‚Üí Checkerboard (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê). If you can get a network to nail the Checkerboard with low loss, you're doing great!</p>
      <p>Each time you select a new dataset (or click Regenerate), 300 random data points are placed on the canvas. The randomness means every run is slightly different ‚Äî which is actually realistic! In real ML, your training data is always a finite, imperfect sample of the true pattern.</p>

      <h3>üèóÔ∏è Architecture (Hidden Layers)</h3>
      <p>This is arguably the most important setting ‚Äî it controls the <strong>shape of your brain</strong>. Specifically, it determines how many <strong>hidden layers</strong> your network has and how many <strong>neurons</strong> are in each one.</p>
      <p>Use the <strong>"+ Layer"</strong> and <strong>"- Layer"</strong> buttons to add or remove layers (up to 6 layers max), and the sliders to set how many neurons each layer has (from 1 to 12). Each layer is labeled L1, L2, L3, etc.</p>
      <p>Why does this matter? A single neuron can only draw a straight line. A single layer of neurons can draw multiple straight lines and combine them into simple shapes. But for complex patterns like spirals or checkerboards, you need multiple layers that build on each other ‚Äî each layer detecting more abstract features than the last.</p>
      <p>Think of it like this: each layer is a level of thinking. One layer can detect simple features ("is the dot on the left or right?"). Two layers can combine those features ("is the dot in the top-left quadrant?"). Three layers can detect even more abstract patterns ("is the dot on this spiral arm?").</p>
      <p><strong>More layers</strong> = can learn more complex patterns, but takes longer to train and needs more data. <strong>More neurons per layer</strong> = more nuance at each thinking level. A common beginner mistake is making the network way too big for a simple problem ‚Äî start small and add complexity only when needed.</p>
      <p><strong>Rule of thumb:</strong> Start with 1‚Äì2 layers of 4 neurons each. If the network can't learn the pattern after training, add more. If it learns too fast and the boundary looks wiggly and overfit, you might have too many neurons ‚Äî try reducing or adding regularization.</p>

      <h3>üìè Learning Rate (LR)</h3>
      <p>If architecture is the most important structural choice, <strong>learning rate</strong> is the most important training choice. It controls how big each weight adjustment step is when the network learns.</p>
      <p>Imagine you're walking toward a target in a dark room, and after each step someone tells you which direction to go:</p>
      <ul>
        <li><strong>High learning rate</strong> (like 0.5 or 1.0) = taking big steps. You'll get close fast, but you might <em>overshoot</em> and keep jumping back and forth past the target.</li>
        <li><strong>Low learning rate</strong> (like 0.001) = taking tiny baby steps. You'll get there eventually, but it takes <em>forever</em>.</li>
        <li><strong>Just right</strong> (usually 0.01‚Äì0.1) = steady progress toward the goal.</li>
      </ul>
      <p><strong>Recommended starting point:</strong> Try <strong>0.05</strong>. This works well for most datasets in this playground. Once you're comfortable, experiment with extremes to see what happens ‚Äî that's the whole point!</p>
      <p>Try cranking the learning rate to 1.0 and watch the loss curve bounce around wildly. Then try 0.001 and watch it barely move. Finding the sweet spot is a real skill in machine learning ‚Äî in industry, people often try dozens of learning rates to find the best one for their specific problem.</p>

      <h3>üîß Activation Function</h3>
      <p>The <strong>activation function</strong> is the "squishing function" each neuron uses to decide its output. Without an activation function, the network would just be doing simple linear math ‚Äî it could only draw straight-line boundaries no matter how many layers you add. The activation function is what gives neural networks their power to learn curves, spirals, and complex shapes.</p>
      <p>You met these in earlier chapters, but here's a refresher:</p>
      <ul>
        <li><strong>ReLU</strong> (Rectified Linear Unit) ‚Äî The most popular activation in modern deep learning. Dead simple: if the input is positive, pass it through unchanged. If negative, output zero. This creates sharp, angular decision boundaries. It's fast to compute and works great in practice. <em>Start here unless you have a reason not to.</em></li>
        <li><strong>Sigmoid</strong> ‚Äî Squishes everything into a smooth curve between 0 and 1. Creates gentle, rounded boundaries. It was the go-to activation in the 1990s but has fallen out of favor for hidden layers because it can cause "vanishing gradients" ‚Äî the learning signal gets weaker in deeper networks, making training slow.</li>
        <li><strong>Tanh</strong> ‚Äî Squishes everything between -1 and +1. Like sigmoid but centered at zero, which often helps the network learn faster. A solid middle-ground choice. Try comparing Tanh vs Sigmoid on the same dataset ‚Äî you'll often see Tanh converge faster.</li>
      </ul>
      <p><strong>Quick comparison tip:</strong> Try the Checkerboard dataset with each activation and the same architecture. ReLU tends to create blocky, grid-like boundaries (which actually suits checkerboard well!). Sigmoid creates smoother, rounder regions.</p>

      <h3>üõ°Ô∏è L2 Regularization</h3>
      <p><strong>L2 regularization</strong> is a technique that prevents the network from <strong>memorizing</strong> the training data too exactly. It's like telling a student: <em>"Learn the concepts, not the specific answers to the practice test."</em></p>
      <p>Without regularization, a powerful network might create a super-complex, wiggly boundary that perfectly fits every single training dot ‚Äî including the noisy or misleading ones. This is called <strong>overfitting</strong>, and it's one of the biggest problems in machine learning. A network that overfits has essentially memorized the answers instead of learning the pattern ‚Äî like a student who memorizes the answer key but can't solve a new problem.</p>
      <p>L2 regularization adds a small penalty for having large weights, which encourages the network to keep things simpler. Mathematically, it adds the sum of all squared weights (multiplied by a small number) to the loss. The network is now trying to minimize <em>two</em> things at once: the prediction error AND the size of its weights. This forces it to find the simplest boundary that still works.</p>
      <ul>
        <li><strong>L2 = 0</strong> ‚Äî No regularization. The network can go wild.</li>
        <li><strong>L2 = small value (0.001‚Äì0.005)</strong> ‚Äî Gentle nudge toward simplicity. Usually a good idea.</li>
        <li><strong>L2 = large value (0.01+)</strong> ‚Äî Strong simplicity constraint. The boundary will be very smooth, but might be <em>too</em> simple to fit the data well (called <strong>underfitting</strong>).</li>
      </ul>
      <p>The sweet spot depends on your network size and dataset complexity. A big network on a simple dataset needs more regularization. A small network on a hard dataset might not need any ‚Äî it's already constrained by its size. Experiment and watch the boundary!</p>

      <div class="callout" style="background:rgba(255,255,255,0.05);border-left:3px solid var(--accent);padding:12px 16px;border-radius:8px;margin:16px 0;">
        <strong>üí° Quick Reference Card</strong>
        <ul style="margin:8px 0 0 0;">
          <li><strong>Network too simple?</strong> ‚Üí Add layers or neurons</li>
          <li><strong>Loss bouncing wildly?</strong> ‚Üí Lower the learning rate</li>
          <li><strong>Loss barely moving?</strong> ‚Üí Raise the learning rate, or the network is too small</li>
          <li><strong>Boundary too wiggly?</strong> ‚Üí Increase L2 regularization or reduce network size</li>
          <li><strong>Boundary too smooth?</strong> ‚Üí Decrease L2 regularization or add more neurons</li>
          <li><strong>Training seems stuck?</strong> ‚Üí Reset and try again (random initialization matters!)</li>
        </ul>
      </div>
    </div>

    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <!-- THE PLAYGROUND -->
    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <div class="section fade-in-up delay-1">
      <h2>üß™ The Playground</h2>
      <p>Alright, you know what everything does ‚Äî now go play! Hit <strong>‚ñ∂ Train</strong> to start training, <strong>‚è∏ Pause</strong> to freeze training so you can examine the boundary, and <strong>Reset Network</strong> to start fresh with new random weights (the data stays the same). Use <strong>Regenerate</strong> to create a new random set of data points for the current dataset type.</p>
      <p><strong>Pro tip:</strong> Pause the training periodically to study the decision boundary. How does it compare to what you'd draw by hand? Are there regions where the network is clearly wrong? What would help ‚Äî more neurons, more layers, or different settings?</p>
      <p><strong>Remember:</strong> Every time you hit "Reset Network," the weights are randomized again. This means two identical networks can learn differently! If training seems stuck or weird, just reset and try again ‚Äî sometimes you just got unlucky with the initial random weights.</p>
    </div>

    <div class="interactive-area fade-in-up delay-1">
      <div class="playground-layout">
        <!-- Left sidebar: controls -->
        <div class="playground-sidebar">
          <div class="card" style="padding:16px;">
            <h3 style="font-size:0.9rem;margin-bottom:12px;">Dataset</h3>
            <select id="pg-dataset" style="width:100%;">
              <option value="circle">Circle</option>
              <option value="xor">XOR</option>
              <option value="spiral" selected>Spiral</option>
              <option value="gaussian">Gaussian</option>
              <option value="checkerboard">Checkerboard</option>
            </select>
            <button class="btn btn-sm btn-secondary" id="pg-regen" style="width:100%;margin-top:8px;">Regenerate</button>
          </div>

          <div class="card" style="padding:16px;">
            <h3 style="font-size:0.9rem;margin-bottom:12px;">Architecture</h3>
            <div id="pg-layers-config">
              <!-- Dynamic layer controls -->
            </div>
            <div style="display:flex;gap:6px;margin-top:8px;">
              <button class="btn btn-sm btn-secondary" id="pg-add-layer" style="flex:1;">+ Layer</button>
              <button class="btn btn-sm btn-secondary" id="pg-remove-layer" style="flex:1;">- Layer</button>
            </div>
          </div>

          <div class="card" style="padding:16px;">
            <h3 style="font-size:0.9rem;margin-bottom:12px;">Settings</h3>
            <div class="slider-group">
              <label style="min-width:auto;font-size:0.75rem;">LR</label>
              <input type="range" id="pg-lr" min="0.001" max="1" step="0.001" value="0.05">
              <span class="slider-value" id="pg-lr-val" style="min-width:40px;">0.05</span>
            </div>
            <div class="control-group" style="margin-top:8px;">
              <label style="font-size:0.75rem;">Activation:</label>
              <select id="pg-activation" style="font-size:0.75rem;padding:4px 8px;">
                <option value="relu" selected>ReLU</option>
                <option value="sigmoid">Sigmoid</option>
                <option value="tanh">Tanh</option>
              </select>
            </div>
            <div class="slider-group" style="margin-top:8px;">
              <label style="min-width:auto;font-size:0.75rem;">L2</label>
              <input type="range" id="pg-l2" min="0" max="0.01" step="0.0001" value="0">
              <span class="slider-value" id="pg-l2-val" style="min-width:40px;">0</span>
            </div>
          </div>

          <div class="controls" style="flex-direction:column;">
            <button class="btn btn-primary" id="pg-train" style="width:100%;">‚ñ∂ Train</button>
            <button class="btn btn-secondary btn-sm" id="pg-reset" style="width:100%;">Reset Network</button>
          </div>

          <div class="stats" style="flex-direction:column;gap:8px;">
            <div class="stat"><div class="stat-value" id="pg-epoch">0</div><div class="stat-label">Epoch</div></div>
            <div class="stat"><div class="stat-value" id="pg-loss">‚Äî</div><div class="stat-label">Loss</div></div>
            <div class="stat"><div class="stat-value" id="pg-params">‚Äî</div><div class="stat-label">Parameters</div></div>
          </div>
        </div>

        <!-- Main: decision boundary -->
        <div class="playground-main">
          <canvas id="pg-canvas" width="500" height="500"></canvas>
        </div>

        <!-- Right sidebar: network viz + narration -->
        <div class="playground-sidebar">
          <div class="card" style="padding:16px;">
            <h3 style="font-size:0.9rem;margin-bottom:8px;">Network</h3>
            <canvas id="pg-network" width="260" height="220"></canvas>
          </div>
          <div class="card" style="padding:16px;">
            <h3 style="font-size:0.9rem;margin-bottom:8px;">Loss</h3>
            <canvas id="pg-loss-chart" width="260" height="130"></canvas>
          </div>
          <div class="narration" id="pg-narration" style="font-size:0.8rem;">
            Choose a dataset, build your network, and hit Train!
          </div>
        </div>
      </div>
    </div>

    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <!-- SECTION: Try This! -->
    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <div class="section fade-in-up delay-2">
      <div class="collapsible-header" style="font-size:1.1rem;">üß™ Try This!</div><div class="collapsible-body">

      <p>Not sure where to start? Here are some guided experiments that will teach you a ton. Work through them in order ‚Äî each one builds on the intuition from the previous one. Don't just read them ‚Äî actually do each step in the playground above!</p>

      <p>‚è±Ô∏è <strong>Time estimate:</strong> Each experiment takes 2‚Äì5 minutes. The whole set takes about 20 minutes.</p>

      <h3>Experiment 1: The Power of Layers</h3>
      <ol>
        <li>Select the <strong>Spiral</strong> dataset.</li>
        <li>Remove layers until you have just <strong>1 hidden layer</strong> with 4 neurons.</li>
        <li>Hit Train. Watch it struggle ‚Äî the boundary can't twist enough to follow the spiral.</li>
        <li>Now reset, add a second layer (+ Layer button), and train again. Notice the improvement.</li>
        <li>Add a third layer. Now the network can trace the spiral beautifully!</li>
      </ol>
      <p><strong>Lesson:</strong> More layers let the network draw increasingly complex boundaries. But each layer adds training time and parameters.</p>

      <h3>Experiment 2: Learning Rate Gone Wrong</h3>
      <ol>
        <li>Use the <strong>Circle</strong> dataset with 2 layers of 4 neurons each.</li>
        <li>Set the learning rate to <strong>1.0</strong> (drag the LR slider all the way right).</li>
        <li>Hit Train. Watch the loss curve ‚Äî it bounces wildly instead of going down smoothly!</li>
        <li>Reset. Now set LR to <strong>0.001</strong>. Train. The loss drops... but painfully slowly.</li>
        <li>Reset. Try <strong>0.05</strong>. Smooth, steady learning. Just right!</li>
      </ol>
      <p><strong>Lesson:</strong> Learning rate is one of the most important settings in machine learning. Too high and the network can't settle down ‚Äî it keeps overshooting the optimal weights. Too low and you'll be waiting all day. In practice, ML engineers often start with a moderate learning rate and decrease it over time (called a "learning rate schedule"), but for this playground, a fixed rate works fine.</p>

      <p><em>Fun fact: In real-world deep learning, there are entire research papers dedicated just to figuring out better ways to set the learning rate. It's that important!</em></p>

      <h3>Experiment 3: Overfitting vs. Regularization</h3>
      <ol>
        <li>Select the <strong>Gaussian</strong> dataset (two overlapping blobs).</li>
        <li>Use 3 layers with lots of neurons (like 8, 8, 8). Keep L2 at 0.</li>
        <li>Train for a while. Notice how the boundary gets <em>really</em> wiggly, trying to perfectly classify every single dot ‚Äî even the ones deep in the "wrong" territory.</li>
        <li>Now reset, set <strong>L2 to 0.005</strong>, and train again. The boundary is smoother and more sensible ‚Äî it accepts that some dots near the overlap are just hard to classify.</li>
      </ol>
      <p><strong>Lesson:</strong> A network that's too powerful will memorize noise. Regularization keeps it honest. This is one of the most important concepts in all of machine learning ‚Äî the balance between a model that's too simple (underfitting) and one that's too complex (overfitting).</p>

      <h3>Experiment 4: Activation Function Showdown</h3>
      <ol>
        <li>Select the <strong>Checkerboard</strong> dataset with 3 layers (6, 6, 4).</li>
        <li>Set activation to <strong>ReLU</strong>. Train for ~2000 epochs. Pause and look at the boundary shape ‚Äî notice the sharp, angular edges.</li>
        <li>Reset. Switch to <strong>Sigmoid</strong>, same architecture. Train again. The boundary is rounder, smoother ‚Äî and it might learn slower.</li>
        <li>Reset. Try <strong>Tanh</strong>. Often the fastest to converge on this dataset.</li>
      </ol>
      <p><strong>Lesson:</strong> The activation function shapes the "vocabulary" of boundaries the network can draw. Different activations suit different problems.</p>

      <h3>Experiment 5: The Impossible Checkerboard</h3>
      <ol>
        <li>Select the <strong>Checkerboard</strong> dataset.</li>
        <li>Start with 1 layer, 4 neurons. Train. It barely makes a dent!</li>
        <li>Try 3 layers (8, 6, 4) with ReLU. Train for a while. It should start to carve out the checkerboard pattern, though it's tough.</li>
        <li>This is one of the hardest patterns ‚Äî it shows why some problems need really deep networks.</li>
        <li>If you're feeling ambitious, try to get the loss below 0.05. It's possible but takes patience and the right architecture!</li>
      </ol>
      <p><strong>Lesson:</strong> Some patterns are genuinely hard, and no amount of clever settings can replace having enough network capacity. When your network can't learn, sometimes the answer is simply "make it bigger."</p>

      <h3>Experiment 6: The Minimalist Challenge</h3>
      <ol>
        <li>Select the <strong>Circle</strong> dataset ‚Äî the easiest one.</li>
        <li>Use just <strong>1 hidden layer with 2 neurons</strong>. Can it learn the circle? (Probably not well!)</li>
        <li>Try 1 layer with 3 neurons. Better?</li>
        <li>What's the <em>smallest</em> network that can perfectly classify the circle? Try to find it!</li>
      </ol>
      <p><strong>Lesson:</strong> In real ML, smaller models are better ‚Äî they're faster, use less memory, and generalize better. Finding the smallest network that solves your problem is an art. This principle is called <strong>Occam's Razor</strong> in machine learning ‚Äî prefer the simplest model that explains the data.</p>

      <h3>üéØ Create Your Own Challenge</h3>
      <p>Once you've done the guided experiments, try setting your own goals:</p>
      <ul>
        <li>Can you solve Spiral with loss below 0.01? What's the smallest network that achieves it?</li>
        <li>Can you find a learning rate that works for ALL datasets without changing it?</li>
        <li>What happens if you set every layer to just 1 neuron? Can any dataset be learned?</li>
        <li>Try training, pausing, changing the learning rate, then resuming. Does a "learning rate schedule" (starting high, going low) help?</li>
        <li>What's the maximum number of parameters you can create? (Hint: 6 layers of 12 neurons each!) Does more always mean better?</li>
      </ul>
    </div>

    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    </div>
    <!-- SECTION: What You Just Learned -->
    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <div class="section fade-in-up delay-2">
      <h2>üìù What You Just Learned</h2>

      <p>If you've worked through even a few of those experiments, you've already developed real intuition about neural networks. Let's crystallize the key takeaways:</p>

      <ul>
        <li>A neural network <strong>playground</strong> lets you experiment with different datasets, architectures, and settings to build intuition about how networks learn.</li>
        <li>The <strong>decision boundary</strong> (the colored canvas) shows you what the network has learned ‚Äî where it draws the line between the two classes.</li>
        <li>The <strong>loss curve</strong> tells you how the network is progressing. Down = good. Flat near zero = done learning. Bouncing = learning rate might be too high.</li>
        <li><strong>Harder datasets</strong> (spiral, checkerboard) need deeper networks with more layers and neurons.</li>
        <li>The <strong>learning rate</strong> is a critical knob ‚Äî too high and the network overshoots, too low and it crawls.</li>
        <li><strong>L2 regularization</strong> prevents overfitting by encouraging simpler boundaries.</li>
        <li>There's no single "best" network for everything ‚Äî the right architecture depends on the problem. <strong>Experimentation is key!</strong></li>
        <li>Different <strong>activation functions</strong> create different boundary shapes ‚Äî ReLU makes angular boundaries, Sigmoid/Tanh make smoother ones.</li>
        <li>The <strong>parameter count</strong> tells you how complex your model is. More parameters = more expressive, but also more prone to overfitting and slower to train.</li>
        <li>Finding the right combination of settings is more art than science ‚Äî that's why playgrounds like this are so valuable for building intuition.</li>
        <li><strong>Random initialization matters</strong> ‚Äî the same network trained twice can learn differently because it starts from different random weights. If training seems stuck, try resetting!</li>
      </ul>

      <h3>üó∫Ô∏è The Bigger Picture</h3>
      <p>Everything you've experimented with here ‚Äî choosing architectures, tuning learning rates, fighting overfitting ‚Äî is exactly what machine learning engineers do at companies like Google, OpenAI, and Meta. The only differences are scale (they use millions of neurons instead of dozens) and data (they train on massive real-world datasets instead of 300 colored dots). But the principles? Identical.</p>
      <p>The fact that you can now look at a loss curve and say "learning rate is too high" or see a wiggly boundary and think "needs regularization" means you've developed <strong>real ML intuition</strong>. That's not something you can learn from reading a textbook ‚Äî it comes from doing, from playing, from failing and trying again. Which is exactly what you just did.</p>

      <p>These aren't just toy concepts ‚Äî every single one of these settings exists in the real neural networks that power ChatGPT, image recognition, self-driving cars, and more. The networks are bigger (millions or billions of parameters instead of dozens), but the fundamental ideas are identical. You're learning the real thing.</p>

      <p>You now have hands-on experience with the core concepts of neural networks: neurons, layers, activation functions, learning rates, and regularization. These are the building blocks of <em>every</em> neural network ever built ‚Äî from the tiny playground networks you just trained to the massive language models that power AI assistants.</p>

      <p>In the next chapter, we'll jump from numbers to <strong>words</strong> ‚Äî and see how neural networks can understand language. How do you turn a sentence into numbers that a network can process? How does a neural network learn the meaning of words? üöÄ</p>
    </div>
  </div>

  <script src="/js/particles.js"></script>
  <script src="/js/neural-engine.js"></script>
  <script src="/js/viz.js"></script>
  <script src="/js/nav.js"></script>
  <script>
    const ps = new ParticleSystem(); ps.start();
    const $ = id => document.getElementById(id);
    const { NeuralNetwork, Datasets } = NeuralEngine;

    let hiddenLayers = [4, 4];
    let pgNet, pgData, pgHistory, pgTraining = false, pgAnimId;

    function renderLayerConfig() {
      const container = $('pg-layers-config');
      container.innerHTML = hiddenLayers.map((n, i) => `
        <div class="slider-group" style="margin:4px 0;">
          <label style="min-width:auto;font-size:0.7rem;">L${i + 1}</label>
          <input type="range" min="1" max="12" value="${n}" data-layer="${i}" class="layer-slider" style="flex:1;">
          <span class="slider-value" style="min-width:20px;font-size:0.8rem;">${n}</span>
        </div>
      `).join('');

      container.querySelectorAll('.layer-slider').forEach(s => {
        s.addEventListener('input', (e) => {
          const idx = parseInt(e.target.dataset.layer);
          hiddenLayers[idx] = parseInt(e.target.value);
          e.target.nextElementSibling.textContent = e.target.value;
        });
      });
    }

    $('pg-add-layer').addEventListener('click', () => {
      if (hiddenLayers.length < 6) {
        hiddenLayers.push(4);
        renderLayerConfig();
      }
    });

    $('pg-remove-layer').addEventListener('click', () => {
      if (hiddenLayers.length > 1) {
        hiddenLayers.pop();
        renderLayerConfig();
      }
    });

    function initPlayground() {
      const activation = $('pg-activation').value;
      const datasetName = $('pg-dataset').value;
      const topology = [2, ...hiddenLayers, 1];
      pgNet = new NeuralNetwork(topology, activation, 'sigmoid');
      pgData = Datasets[datasetName](300);
      pgHistory = [];
      pgTraining = false;
      $('pg-train').textContent = '‚ñ∂ Train';
      $('pg-params').textContent = pgNet.paramCount();
      $('pg-epoch').textContent = '0';
      $('pg-loss').textContent = '‚Äî';
      drawPlayground();
    }

    function drawPlayground() {
      const canvas = $('pg-canvas'), ctx = canvas.getContext('2d');
      ctx.clearRect(0, 0, canvas.width, canvas.height);

      const grid = pgNet.classifyGrid(60, -6, 6, -6, 6);
      Viz.drawDecisionBoundary(ctx, grid, 60, canvas.width, canvas.height);
      Viz.drawDataPoints(ctx, pgData, canvas.width, canvas.height);

      // Network viz
      const nc = $('pg-network'), nctx = nc.getContext('2d');
      Viz.drawNetwork(nctx, pgNet, nc.width, nc.height, [0, 0]);

      // Loss chart
      const lc = $('pg-loss-chart'), lctx = lc.getContext('2d');
      Viz.drawLossChart(lctx, pgHistory, lc.width, lc.height);

      $('pg-epoch').textContent = pgNet.epoch;
      $('pg-loss').textContent = pgNet.loss === Infinity ? '‚Äî' : pgNet.loss.toFixed(5);
    }

    function trainStep() {
      if (!pgTraining) return;
      const lr = parseFloat($('pg-lr').value);
      const l2 = parseFloat($('pg-l2').value);

      for (let i = 0; i < 5; i++) {
        const loss = pgNet.trainBatch(pgData, lr, 'mse', l2);
        pgHistory.push(loss);
      }

      drawPlayground();
      updateNarration();

      if (pgNet.epoch < 10000) {
        pgAnimId = requestAnimationFrame(trainStep);
      } else {
        pgTraining = false;
        $('pg-train').textContent = '‚ñ∂ Train';
      }
    }

    function updateNarration() {
      const epoch = pgNet.epoch;
      const loss = pgNet.loss;
      let msg = '';

      if (epoch < 50) {
        msg = `<strong>Starting up!</strong> The network is randomly initialized. Loss is ${loss.toFixed(4)} ‚Äî it's just guessing.`;
      } else if (loss > 0.2) {
        msg = `<strong>Learning...</strong> Loss is ${loss.toFixed(4)}. The decision boundary is starting to take shape, but there's a lot to learn.`;
      } else if (loss > 0.05) {
        msg = `<strong>Getting there!</strong> Loss is down to ${loss.toFixed(4)}. The boundary is clearly separating the two classes.`;
      } else if (loss > 0.01) {
        msg = `<strong>Almost!</strong> Loss is ${loss.toFixed(5)}. Fine-tuning the boundary for the tricky points.`;
      } else {
        msg = `<strong>üéâ Excellent!</strong> Loss is ${loss.toFixed(5)}. The network has learned this dataset well!`;
        Nav.setCompleted(4);
      }

      $('pg-narration').innerHTML = msg;
    }

    $('pg-train').addEventListener('click', () => {
      if (!pgNet) initPlayground();
      pgTraining = !pgTraining;
      $('pg-train').textContent = pgTraining ? '‚è∏ Pause' : '‚ñ∂ Train';
      if (pgTraining) trainStep();
    });

    $('pg-reset').addEventListener('click', () => {
      pgTraining = false;
      if (pgAnimId) cancelAnimationFrame(pgAnimId);
      initPlayground();
    });

    $('pg-regen').addEventListener('click', () => {
      pgData = Datasets[$('pg-dataset').value](300);
      drawPlayground();
    });

    $('pg-dataset').addEventListener('change', () => {
      pgTraining = false;
      if (pgAnimId) cancelAnimationFrame(pgAnimId);
      initPlayground();
    });

    $('pg-lr').addEventListener('input', () => {
      $('pg-lr-val').textContent = parseFloat($('pg-lr').value).toFixed(3);
    });

    $('pg-l2').addEventListener('input', () => {
      $('pg-l2-val').textContent = parseFloat($('pg-l2').value).toFixed(4);
    });

    renderLayerConfig();
    initPlayground();
  </script>
</body>
</html>
