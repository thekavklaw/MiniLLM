<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 1: What IS a Neural Network? ‚Äî MiniLLM</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>üß†</text></svg>">
  <link rel="stylesheet" href="/css/style.css">
</head>
<body data-chapter="1">
  <nav class="chapter-nav">
    <a href="/">‚Üê Home</a>
    <span class="nav-title">Chapter 1: The Neuron</span>
    <a href="/chapters/2-learning.html">Next ‚Üí</a>
  </nav>
  <div class="chapter-content">
    <div class="chapter-header fade-in-up">
      <div class="chapter-number">Chapter 1</div>
      <h1>What IS a Neural Network?</h1>
      <p>You've used ChatGPT. You've watched it write poems, answer questions, and even crack jokes. But have you ever wondered what's actually happening inside? Let's peek under the hood and meet the tiny building block that makes all of AI possible: <strong>the neuron</strong>.</p>
    </div>

    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <!-- SECTION 1: What is a neuron?                -->
    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <div class="section fade-in-up delay-1">
      <h2>üß† A Neuron is a Decision-Maker</h2>

      <p>Before we get into any math, let's build an analogy. Imagine you're at a town meeting, and there's a vote on whether to build a new park. Each person at the meeting shares their opinion ‚Äî some are enthusiastic ("Yes! Build it!"), others are skeptical ("I don't think so"). Now imagine there's one person ‚Äî let's call them <strong>the Voter</strong> ‚Äî who has to listen to everyone and make a final decision.</p>

      <p>Here's the thing about the Voter: they don't trust everyone equally. Their best friend's opinion carries a lot of weight, but the stranger in the back row? Not so much. The Voter also has their own <strong>pre-existing opinion</strong> (a gut feeling) before hearing anyone else. After weighing all the opinions, the Voter makes a decision: thumbs up or thumbs down.</p>

      <p><strong>That's exactly what a neuron does.</strong> A neuron in a neural network is a tiny mathematical decision-maker. It takes in numbers (the opinions), multiplies each one by how much it "trusts" that input (the <strong>weight</strong> ‚Äî a number that says how important each input is), adds them all up, throws in its own gut feeling (the <strong>bias</strong> ‚Äî a number that shifts the neuron's default preference), and then makes a decision.</p>

      <p>But wait ‚Äî there's one more step. The raw sum could be any number: -500, 0, a million. That's not very useful. We need the neuron to give us something manageable, like a number between 0 and 1, where 0 means "definitely no" and 1 means "definitely yes." That's where the <strong>activation function</strong> comes in.</p>

      <h3>üéöÔ∏è The Activation Function: A Volume Knob</h3>

      <p>Think of the <strong>activation function</strong> as a volume knob on a speaker. No matter how loud the raw signal is (whether it's a whisper or a scream), the volume knob squishes it into a useful range so your ears don't explode and you can still hear something.</p>

      <p>The most common activation function for beginners is called the <strong>sigmoid function</strong> (pronounced "SIG-moyd"). Sigmoid takes <em>any</em> number ‚Äî positive, negative, huge, tiny ‚Äî and squishes it into a value between 0 and 1. Here are some examples:</p>

      <ul>
        <li><strong>-100</strong> ‚Üí sigmoid gives <strong>0.0</strong> (basically zero ‚Äî the neuron is "off")</li>
        <li><strong>0</strong> ‚Üí sigmoid gives <strong>0.5</strong> (right in the middle ‚Äî the neuron is undecided)</li>
        <li><strong>+100</strong> ‚Üí sigmoid gives <strong>1.0</strong> (basically one ‚Äî the neuron is fully "on")</li>
      </ul>

      <p>See the pattern? Very negative numbers get squished close to 0. Very positive numbers get squished close to 1. And anything near zero lands around 0.5. This "squishing" is what makes the neuron's output interpretable ‚Äî you can think of it as a confidence level. 0.9 means "I'm 90% sure this is a yes!" and 0.1 means "I'm 90% sure this is a no."</p>

      <h3>üìê The Formula (Don't Panic!)</h3>

      <p>Now that we understand all the pieces, here's the complete formula for what a single neuron does:</p>

      <p style="text-align:center; font-size:1.25em; margin:24px 0; padding:16px; background:rgba(139,92,246,0.08); border-radius:12px;">
        <strong>output = activation( weight √ó input + bias )</strong>
      </p>

      <p>Let's break that down one more time, left to right:</p>
      <ol>
        <li><strong>input</strong> ‚Äî the number coming in (someone's opinion at the meeting)</li>
        <li><strong>weight</strong> ‚Äî how much the neuron trusts that input (how much the Voter values that person's opinion). A high weight = "I really listen to you." A weight near zero = "I'm ignoring you."</li>
        <li><strong>weight √ó input</strong> ‚Äî multiply them together. This is the "weighted opinion."</li>
        <li><strong>+ bias</strong> ‚Äî add the neuron's own gut feeling. A positive bias means the neuron <em>wants</em> to say yes. A negative bias means it's skeptical by default.</li>
        <li><strong>activation()</strong> ‚Äî squish the result through sigmoid so we get a nice number between 0 and 1.</li>
      </ol>

      <p>That's it. That's the entire brain cell of artificial intelligence. Every AI ‚Äî from the one that recommends your next TikTok video to ChatGPT itself ‚Äî is built from millions (or billions) of these tiny voters, all wired together.</p>
    </div>

    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <!-- SECTION 2: Interactive neuron               -->
    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <div class="section fade-in-up delay-2">
      <h2>üéõÔ∏è Interactive: Your First Neuron</h2>

      <p>Enough reading ‚Äî let's <em>play</em>. Below is a live neuron you can control. There are three sliders: <strong>Input</strong>, <strong>Weight</strong>, and <strong>Bias</strong>. Drag them around and watch what happens to the neuron's output in real time.</p>

      <p>On the right, you'll see a visual diagram: the input flows in from the left, gets multiplied by the weight along the connection line, enters the neuron (the big purple circle), gets the bias added, and then passes through the sigmoid curve at the bottom. The orange dot on the sigmoid curve shows where your current weighted sum lands.</p>

      <div class="collapsible-header">üí° Things to try</div><div class="collapsible-body">
      <ul>
        <li>Set the weight to <strong>0</strong>. What happens? (The neuron ignores the input entirely!)</li>
        <li>Set the weight to a <strong>negative number</strong>. Now higher inputs make the output <em>lower</em>. The neuron is "anti-correlated" with the input.</li>
        <li>Crank the bias to <strong>+5</strong>. The neuron is always excited, no matter what the input is ‚Äî it has a very strong gut feeling of "yes!"</li>
        <li>Set a large weight (like 3) and sweep the input from -5 to 10. Watch how the sigmoid curve creates a sharp "switch" from 0 to 1.</li>
      </ul>
</div>

      <div class="interactive-area">
        <div class="grid-2">
          <div>
            <div class="slider-group">
              <label>Input</label>
              <input type="range" id="input1" min="-5" max="10" step="0.1" value="3">
              <span class="slider-value" id="input1-val">3.0</span>
            </div>
            <div class="slider-group">
              <label>Weight</label>
              <input type="range" id="weight1" min="-3" max="3" step="0.1" value="1">
              <span class="slider-value" id="weight1-val">1.0</span>
            </div>
            <div class="slider-group">
              <label>Bias</label>
              <input type="range" id="bias1" min="-5" max="5" step="0.1" value="0">
              <span class="slider-value" id="bias1-val">0.0</span>
            </div>
            <div class="stats" style="margin-top:24px;">
              <div class="stat">
                <div class="stat-value" id="weighted-sum">3.0</div>
                <div class="stat-label">Weighted Sum</div>
              </div>
              <div class="stat">
                <div class="stat-value" id="neuron-output">0.95</div>
                <div class="stat-label">Output (Sigmoid)</div>
              </div>
            </div>
            <div class="narration" id="narration1">
              The neuron received <strong>3.0</strong>, multiplied by weight <strong>1.0</strong>, added bias <strong>0.0</strong> to get <strong>3.0</strong>. After sigmoid, it outputs <strong>0.95</strong> ‚Äî very excited!
            </div>
          </div>
          <div>
            <canvas id="neuron-canvas" width="400" height="350"></canvas>
          </div>
        </div>
      </div>

      <p>Notice how the <strong>weighted sum</strong> (weight √ó input + bias) can be any number, but the <strong>output</strong> is always between 0 and 1 thanks to sigmoid. That's the activation function doing its job ‚Äî squishing the result into a range we can interpret as a confidence level.</p>
    </div>

    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <!-- SECTION 3: Activity                         -->
    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <div class="section fade-in-up delay-3">
      <h2>üéØ Activity: Build a "Greater Than 5" Detector</h2>

      <p>Here's your first real challenge. Can you turn the neuron above into a <strong>number classifier</strong>? Specifically, we want it to output a number <strong>close to 1</strong> (above 0.6) when the input is greater than 5, and <strong>close to 0</strong> (below 0.4) when the input is 5 or less.</p>

      <p>In other words, we want our neuron to act like a simple detector: "Is this number bigger than 5? Yes or no?" If the input is 7, the neuron should say something like 0.95 (yes!). If the input is 2, it should say something like 0.04 (nope).</p>

      <p><strong>How to think about it:</strong></p>
      <ul>
        <li>The <strong>weight</strong> controls how "steep" the switch is. A higher weight makes the neuron flip sharply from 0 to 1 instead of gradually. Think of it like the difference between a dimmer switch and a light switch ‚Äî you want a sharp cutoff.</li>
        <li>The <strong>bias</strong> controls <em>where</em> the switch happens. Since we want the switch at input = 5, you need to set the bias so that when input = 5, the weighted sum is near 0 (because sigmoid(0) = 0.5, right at the boundary).</li>
        <li>Quick math: if weight = 3 and you want weight √ó 5 + bias = 0, then bias = -15. Try values around there!</li>
      </ul>

      <p>When you're ready, click <strong>Test My Neuron</strong> and it will check all inputs from 0 to 10. You need all 11 to pass. Don't worry if it takes a few tries ‚Äî that's how learning works (for humans too!).</p>

      <div class="interactive-area">
        <div id="activity-status" class="narration">Adjust the sliders above, then click "Test My Neuron" to see how you did!</div>
        <div style="margin-top:12px;">
          <button class="btn btn-primary btn-sm" id="test-btn">Test My Neuron</button>
          <button class="btn btn-secondary btn-sm" id="hint-btn">Show Hint</button>
        </div>
      </div>
    </div>

    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <!-- SECTION 4: Why does this matter?            -->
    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <div class="section fade-in-up">
      <h2>üí° Why Does This Matter?</h2>

      <p>You might be thinking: "Okay, cool, one neuron can detect if a number is bigger than 5. But ChatGPT writes entire essays. How do we get from <em>here</em> to <em>there</em>?"</p>

      <p>Great question. The secret is that <strong>ChatGPT is made of billions of these neurons</strong>, all connected together in layers. Each neuron on its own is simple ‚Äî it just multiplies, adds, and squishes. But when you wire millions of them together, something magical happens: the network can learn incredibly complex patterns. One neuron might detect "is this word a noun?", another might detect "does this sentence sound formal?", and together, layer after layer, they build up an understanding of language.</p>

      <p>Think of it like LEGO bricks. One brick is boring. But with enough of them, you can build a castle, a spaceship, or anything you can imagine. The neuron is your LEGO brick. In the next chapters, we'll start connecting them together and teaching them to <em>learn</em>.</p>

      <p style="text-align:center; padding:20px; background:rgba(139,92,246,0.06); border-radius:12px; margin-top:16px;">
        <strong>üß± One neuron = one LEGO brick.<br>
        Billions of neurons wired together = ChatGPT.</strong><br>
        <span style="opacity:0.7;">Next up: how does a neuron <em>learn</em> the right weights?</span>
      </p>
    </div>
  </div>

  <script src="/js/particles.js"></script>
  <script src="/js/neural-engine.js"></script>
  <script src="/js/viz.js"></script>
  <script src="/js/nav.js"></script>
  <script>
    const ps = new ParticleSystem(); ps.start();

    const $ = id => document.getElementById(id);
    const neuron = new NeuralEngine.SingleNeuron(1, 'sigmoid');

    const canvas = $('neuron-canvas');
    const ctx = canvas.getContext('2d');

    function update() {
      const input = parseFloat($('input1').value);
      const weight = parseFloat($('weight1').value);
      const bias = parseFloat($('bias1').value);

      $('input1-val').textContent = input.toFixed(1);
      $('weight1-val').textContent = weight.toFixed(1);
      $('bias1-val').textContent = bias.toFixed(1);

      neuron.weights[0] = weight;
      neuron.bias = bias;
      const output = neuron.forward([input]);
      const sum = input * weight + bias;

      $('weighted-sum').textContent = sum.toFixed(2);
      $('neuron-output').textContent = output.toFixed(4);

      // Narration
      const excitement = output > 0.8 ? 'very excited!' : output > 0.5 ? 'mildly interested.' : output > 0.2 ? 'not very interested.' : 'barely activated.';
      $('narration1').innerHTML = `The neuron received <strong>${input.toFixed(1)}</strong>, multiplied by weight <strong>${weight.toFixed(1)}</strong>, added bias <strong>${bias.toFixed(1)}</strong> to get <strong>${sum.toFixed(2)}</strong>. After sigmoid, it outputs <strong>${output.toFixed(4)}</strong> ‚Äî ${excitement}`;

      drawNeuronViz(input, weight, bias, sum, output);
    }

    function drawNeuronViz(input, weight, bias, sum, output) {
      const w = canvas.width, h = canvas.height;
      ctx.clearRect(0, 0, w, h);

      // Input node
      Viz.drawNeuron(ctx, 80, h / 2, 25, Math.abs(input) / 10, Viz.colors.blue);
      ctx.fillStyle = Viz.colors.text;
      ctx.font = '13px Inter, sans-serif';
      ctx.textAlign = 'center';
      ctx.fillText(input.toFixed(1), 80, h / 2 + 42);
      ctx.fillText('Input', 80, h / 2 - 38);

      // Connection
      Viz.drawConnection(ctx, 105, h / 2, 255, h / 2, weight);
      ctx.fillStyle = Viz.colors.textLight;
      ctx.font = '11px Inter, sans-serif';
      ctx.fillText(`w = ${weight.toFixed(1)}`, 180, h / 2 - 12);

      // Neuron
      Viz.drawNeuron(ctx, 280, h / 2, 35, output, Viz.colors.purple);
      ctx.fillStyle = Viz.colors.text;
      ctx.font = '14px Inter, sans-serif';
      ctx.fillText(output.toFixed(3), 280, h / 2 + 55);
      ctx.fillText('Neuron', 280, h / 2 - 50);

      // Bias arrow
      ctx.fillStyle = Viz.colors.orange;
      ctx.font = '11px Inter, sans-serif';
      ctx.fillText(`bias = ${bias.toFixed(1)}`, 280, h / 2 + 72);

      // Output arrow
      ctx.beginPath();
      ctx.moveTo(315, h / 2);
      ctx.lineTo(380, h / 2);
      ctx.strokeStyle = Viz.hexToRgba(Viz.colors.purple, 0.6);
      ctx.lineWidth = 2;
      ctx.stroke();
      ctx.beginPath();
      ctx.moveTo(380, h / 2);
      ctx.lineTo(373, h / 2 - 5);
      ctx.lineTo(373, h / 2 + 5);
      ctx.fillStyle = Viz.colors.purple;
      ctx.fill();

      // Sigmoid curve at bottom
      ctx.save();
      const cx = 40, cy = h - 80, cw = w - 80, ch2 = 60;
      ctx.strokeStyle = '#e2e8f0';
      ctx.lineWidth = 1;
      ctx.beginPath(); ctx.moveTo(cx, cy); ctx.lineTo(cx + cw, cy); ctx.stroke();
      ctx.beginPath(); ctx.moveTo(cx, cy - ch2); ctx.lineTo(cx + cw, cy - ch2); ctx.stroke();

      ctx.beginPath();
      for (let i = 0; i <= cw; i++) {
        const x_val = (i / cw) * 12 - 6;
        const sig = 1 / (1 + Math.exp(-x_val));
        const px = cx + i;
        const py = cy - sig * ch2;
        if (i === 0) ctx.moveTo(px, py); else ctx.lineTo(px, py);
      }
      ctx.strokeStyle = Viz.colors.purple;
      ctx.lineWidth = 2;
      ctx.stroke();

      // Current point on curve
      const normX = (sum + 6) / 12;
      const dotX = cx + normX * cw;
      const dotY = cy - output * ch2;
      ctx.beginPath();
      ctx.arc(Math.max(cx, Math.min(cx + cw, dotX)), dotY, 6, 0, Math.PI * 2);
      ctx.fillStyle = Viz.colors.orange;
      ctx.fill();
      ctx.strokeStyle = '#fff';
      ctx.lineWidth = 2;
      ctx.stroke();

      ctx.fillStyle = Viz.colors.textLight;
      ctx.font = '10px Inter, sans-serif';
      ctx.textAlign = 'left';
      ctx.fillText('Sigmoid activation', cx, cy + 16);
      ctx.restore();
    }

    ['input1', 'weight1', 'bias1'].forEach(id => $(id).addEventListener('input', update));
    update();

    // Activity
    $('test-btn').addEventListener('click', () => {
      const weight = parseFloat($('weight1').value);
      const bias = parseFloat($('bias1').value);
      neuron.weights[0] = weight;
      neuron.bias = bias;

      let correct = 0;
      const total = 11;
      const results = [];
      for (let x = 0; x <= 10; x++) {
        const out = neuron.forward([x]);
        const expected = x > 5 ? 1 : 0;
        const pass = (expected === 1 && out > 0.6) || (expected === 0 && out < 0.4);
        if (pass) correct++;
        results.push(`${x}: ${out.toFixed(2)} ${pass ? '‚úÖ' : '‚ùå'}`);
      }

      const status = $('activity-status');
      if (correct === total) {
        status.innerHTML = `<strong>üéâ Perfect!</strong> Your neuron correctly detects "greater than 5"!<br>${results.join(' | ')}`;
        Nav.setCompleted(1);
      } else {
        status.innerHTML = `<strong>${correct}/${total} correct.</strong> Keep adjusting! Remember: high weight for a sharp cutoff, negative bias to shift the threshold to 5.<br>${results.join(' | ')}`;
      }
    });

    $('hint-btn').addEventListener('click', () => {
      $('activity-status').innerHTML = `<strong>üí° Hint:</strong> Try weight ‚âà 3.0 and bias ‚âà -16.5. A high weight makes the sigmoid steep (sharp cutoff), and the negative bias shifts the threshold to x = 5. The math: weight √ó 5 + bias ‚âà 0, so sigmoid ‚âà 0.5 right at the boundary.`;
    });
  </script>
</body>
</html>
